{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üöÄ Paraphraser App - Google Colab Edition\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
                "\n",
                "Notebook ini berisi:\n",
                "- ‚úÖ Setup otomatis untuk Google Colab\n",
                "- ‚úÖ Model Paraphrasing menggunakan T5\n",
                "- ‚úÖ Evaluasi performa model (BLEU, ROUGE, Inference Time)\n",
                "- ‚úÖ Generate file `web_app.py` otomatis\n",
                "- ‚úÖ Deploy Streamlit di Colab dengan Ngrok\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## üì¶ Step 1: Install Dependencies\n",
                "\n",
                "Install semua package yang diperlukan untuk Colab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install dependencies\n",
                "!pip install -q transformers torch nltk streamlit pyngrok\n",
                "!pip install -q rouge-score sacrebleu\n",
                "\n",
                "print(\"‚úÖ All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "imports"
            },
            "source": [
                "## üìö Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "import_libs"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "import torch\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "import time\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# For evaluation\n",
                "from rouge_score import rouge_scorer\n",
                "from sacrebleu.metrics import BLEU\n",
                "\n",
                "# Download NLTK data\n",
                "try:\n",
                "    nltk.data.find('tokenizers/punkt_tab')\n",
                "    print(\"‚úÖ NLTK punkt_tab already downloaded\")\n",
                "except LookupError:\n",
                "    print(\"üì• Downloading NLTK punkt_tab...\")\n",
                "    nltk.download('punkt_tab', quiet=True)\n",
                "    print(\"‚úÖ Download complete!\")\n",
                "\n",
                "# Check GPU availability\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"\\nüñ•Ô∏è  Using device: {device.upper()}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "load_model"
            },
            "source": [
                "## ü§ñ Step 3: Load Model and Tokenizer\n",
                "\n",
                "Loading model `humarin/chatgpt_paraphraser_on_T5_base` dari Hugging Face"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_loading"
            },
            "outputs": [],
            "source": [
                "print(\"üì• Loading model and tokenizer...\")\n",
                "start_time = time.time()\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"\\n‚úÖ Model loaded successfully in {load_time:.2f} seconds!\")\n",
                "\n",
                "# Model info\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"üìä Model Parameters:\")\n",
                "print(f\"   Total: {total_params:,}\")\n",
                "print(f\"   Trainable: {trainable_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "functions"
            },
            "source": [
                "## ‚öôÔ∏è Step 4: Define Paraphrase Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "define_functions"
            },
            "outputs": [],
            "source": [
                "def paraphrase_sentence(\n",
                "    sentence,\n",
                "    num_beams=5,\n",
                "    num_return_sequences=1,\n",
                "    repetition_penalty=2.0,\n",
                "    no_repeat_ngram_size=2,\n",
                "    temperature=1.0,\n",
                "    max_length=128\n",
                "):\n",
                "    \"\"\"\n",
                "    Paraphrase a single sentence using T5 model.\n",
                "    \n",
                "    Returns:\n",
                "        list: List of paraphrased sentences\n",
                "        float: Inference time in seconds\n",
                "    \"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    input_ids = tokenizer(\n",
                "        f'paraphrase: {sentence}',\n",
                "        return_tensors=\"pt\",\n",
                "        padding=\"longest\",\n",
                "        max_length=max_length,\n",
                "        truncation=True,\n",
                "    ).input_ids.to(device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        input_ids,\n",
                "        temperature=temperature,\n",
                "        repetition_penalty=repetition_penalty,\n",
                "        num_return_sequences=num_return_sequences,\n",
                "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
                "        num_beams=num_beams,\n",
                "        max_length=max_length,\n",
                "        early_stopping=True\n",
                "    )\n",
                "\n",
                "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "    inference_time = time.time() - start_time\n",
                "    \n",
                "    return res, inference_time\n",
                "\n",
                "\n",
                "def paraphrase_paragraph(paragraph, verbose=True):\n",
                "    \"\"\"\n",
                "    Paraphrase an entire paragraph by splitting into sentences.\n",
                "    \n",
                "    Returns:\n",
                "        str: Paraphrased paragraph\n",
                "        float: Total inference time\n",
                "    \"\"\"\n",
                "    sentences = sent_tokenize(paragraph)\n",
                "    if verbose:\n",
                "        print(f\"üìù Processing {len(sentences)} sentences...\")\n",
                "\n",
                "    paraphrased_sentences = []\n",
                "    total_time = 0\n",
                "\n",
                "    for i, sentence in enumerate(sentences, 1):\n",
                "        if verbose:\n",
                "            print(f\"   {i}/{len(sentences)}: Paraphrasing...\", end=\" \")\n",
                "        \n",
                "        paraphrased_result, inf_time = paraphrase_sentence(\n",
                "            sentence,\n",
                "            num_beams=3,\n",
                "            num_return_sequences=1,\n",
                "            max_length=128\n",
                "        )\n",
                "        paraphrased_sentences.append(paraphrased_result[0])\n",
                "        total_time += inf_time\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"‚úÖ ({inf_time:.3f}s)\")\n",
                "\n",
                "    paraphrased_paragraph = \" \".join(paraphrased_sentences)\n",
                "    return paraphrased_paragraph, total_time\n",
                "\n",
                "\n",
                "def calculate_bleu(reference, hypothesis):\n",
                "    \"\"\"Calculate BLEU score\"\"\"\n",
                "    bleu = BLEU()\n",
                "    score = bleu.sentence_score(hypothesis, [reference])\n",
                "    return score.score\n",
                "\n",
                "\n",
                "def calculate_rouge(reference, hypothesis):\n",
                "    \"\"\"Calculate ROUGE scores\"\"\"\n",
                "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
                "    scores = scorer.score(reference, hypothesis)\n",
                "    return {\n",
                "        'rouge1': scores['rouge1'].fmeasure,\n",
                "        'rouge2': scores['rouge2'].fmeasure,\n",
                "        'rougeL': scores['rougeL'].fmeasure\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Functions defined successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_sentence"
            },
            "source": [
                "## üß™ Step 5: Test with Simple Sentence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_simple"
            },
            "outputs": [],
            "source": [
                "test_sentence = \"Artificial Intelligence is transforming the world of technology.\"\n",
                "\n",
                "print(f\"üìå Original: {test_sentence}\")\n",
                "print(\"\\nüîÑ Generating paraphrases...\\n\")\n",
                "\n",
                "results, inf_time = paraphrase_sentence(test_sentence, num_beams=5, num_return_sequences=3)\n",
                "\n",
                "print(\"‚ú® Paraphrased versions:\")\n",
                "for i, result in enumerate(results, 1):\n",
                "    print(f\"   {i}. {result}\")\n",
                "\n",
                "print(f\"\\n‚è±Ô∏è  Inference time: {inf_time:.3f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_paragraph"
            },
            "source": [
                "## üìÑ Step 6: Test with Full Paragraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_para"
            },
            "outputs": [],
            "source": [
                "paragraph = \"\"\"Hugging Face is an innovative AI company that has become a leading platform for natural language processing (NLP) and machine learning tools. Founded in 2016, it started as a chatbot app but soon pivoted to focus on developing open-source models and libraries for NLP tasks. Hugging Face is best known for its Transformers library, which provides pre-trained models for tasks like text classification, translation, summarization, and question-answering.\"\"\"\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"üìå ORIGINAL PARAGRAPH:\")\n",
                "print(\"=\"*80)\n",
                "print(paragraph)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "\n",
                "paraphrased, total_time = paraphrase_paragraph(paragraph)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚ú® PARAPHRASED PARAGRAPH:\")\n",
                "print(\"=\"*80)\n",
                "print(paraphrased)\n",
                "print(\"=\"*80)\n",
                "print(f\"\\n‚è±Ô∏è  Total inference time: {total_time:.3f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "evaluation"
            },
            "source": [
                "## üìä Step 7: Model Performance Evaluation\n",
                "\n",
                "Evaluasi performa model menggunakan BLEU dan ROUGE scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "eval_model"
            },
            "outputs": [],
            "source": [
                "# Test cases untuk evaluasi\n",
                "test_cases = [\n",
                "    \"Machine learning models can learn patterns from data without being explicitly programmed.\",\n",
                "    \"Deep learning is a subset of machine learning that uses neural networks.\",\n",
                "    \"Natural language processing enables computers to understand human language.\",\n",
                "    \"Cloud computing provides on-demand access to computing resources.\",\n",
                "    \"Cybersecurity protects systems and networks from digital attacks.\"\n",
                "]\n",
                "\n",
                "print(\"üéØ Evaluating Model Performance\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "total_bleu = 0\n",
                "total_rouge1 = 0\n",
                "total_rouge2 = 0\n",
                "total_rougeL = 0\n",
                "total_inf_time = 0\n",
                "\n",
                "for i, original in enumerate(test_cases, 1):\n",
                "    print(f\"\\nüìù Test Case {i}:\")\n",
                "    print(f\"   Original: {original}\")\n",
                "    \n",
                "    paraphrased, inf_time = paraphrase_sentence(original, num_beams=5, num_return_sequences=1)\n",
                "    paraphrased = paraphrased[0]\n",
                "    \n",
                "    print(f\"   Paraphrased: {paraphrased}\")\n",
                "    \n",
                "    # Calculate metrics\n",
                "    bleu = calculate_bleu(original, paraphrased)\n",
                "    rouge = calculate_rouge(original, paraphrased)\n",
                "    \n",
                "    total_bleu += bleu\n",
                "    total_rouge1 += rouge['rouge1']\n",
                "    total_rouge2 += rouge['rouge2']\n",
                "    total_rougeL += rouge['rougeL']\n",
                "    total_inf_time += inf_time\n",
                "    \n",
                "    print(f\"   üìä Metrics:\")\n",
                "    print(f\"      BLEU: {bleu:.2f}\")\n",
                "    print(f\"      ROUGE-1: {rouge['rouge1']:.4f}\")\n",
                "    print(f\"      ROUGE-2: {rouge['rouge2']:.4f}\")\n",
                "    print(f\"      ROUGE-L: {rouge['rougeL']:.4f}\")\n",
                "    print(f\"      Inference Time: {inf_time:.3f}s\")\n",
                "\n",
                "n = len(test_cases)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìà AVERAGE PERFORMANCE METRICS\")\n",
                "print(\"=\"*80)\n",
                "print(f\"   Average BLEU Score: {total_bleu/n:.2f}\")\n",
                "print(f\"   Average ROUGE-1: {total_rouge1/n:.4f}\")\n",
                "print(f\"   Average ROUGE-2: {total_rouge2/n:.4f}\")\n",
                "print(f\"   Average ROUGE-L: {total_rougeL/n:.4f}\")\n",
                "print(f\"   Average Inference Time: {total_inf_time/n:.3f}s\")\n",
                "print(f\"   Total Processing Time: {total_inf_time:.3f}s\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Display summary\n",
                "display(HTML(f\"\"\"\n",
                "<div style='background-color: #e8f4f8; padding: 20px; border-radius: 10px; margin-top: 20px;'>\n",
                "    <h3 style='color: #0066cc; margin-top: 0;'>üéØ Model Performance Summary</h3>\n",
                "    <table style='width: 100%; border-collapse: collapse;'>\n",
                "        <tr style='background-color: #d0e8f0;'>\n",
                "            <th style='padding: 10px; text-align: left; border: 1px solid #0066cc;'>Metric</th>\n",
                "            <th style='padding: 10px; text-align: center; border: 1px solid #0066cc;'>Score</th>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td style='padding: 10px; border: 1px solid #ccc;'>BLEU Score</td>\n",
                "            <td style='padding: 10px; text-align: center; border: 1px solid #ccc; font-weight: bold;'>{total_bleu/n:.2f}</td>\n",
                "        </tr>\n",
                "        <tr style='background-color: #f9f9f9;'>\n",
                "            <td style='padding: 10px; border: 1px solid #ccc;'>ROUGE-1 F1</td>\n",
                "            <td style='padding: 10px; text-align: center; border: 1px solid #ccc; font-weight: bold;'>{total_rouge1/n:.4f}</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td style='padding: 10px; border: 1px solid #ccc;'>ROUGE-2 F1</td>\n",
                "            <td style='padding: 10px; text-align: center; border: 1px solid #ccc; font-weight: bold;'>{total_rouge2/n:.4f}</td>\n",
                "        </tr>\n",
                "        <tr style='background-color: #f9f9f9;'>\n",
                "            <td style='padding: 10px; border: 1px solid #ccc;'>ROUGE-L F1</td>\n",
                "            <td style='padding: 10px; text-align: center; border: 1px solid #ccc; font-weight: bold;'>{total_rougeL/n:.4f}</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td style='padding: 10px; border: 1px solid #ccc;'>Avg Inference Time</td>\n",
                "            <td style='padding: 10px; text-align: center; border: 1px solid #ccc; font-weight: bold;'>{total_inf_time/n:.3f}s</td>\n",
                "        </tr>\n",
                "    </table>\n",
                "</div>\n",
                "\"\"\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "custom_test"
            },
            "source": [
                "## ‚úèÔ∏è Step 8: Try Your Own Text!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "custom_input"
            },
            "outputs": [],
            "source": [
                "# Edit variabel ini dengan teks Anda sendiri\n",
                "your_text = \"The advancement of artificial intelligence has revolutionized many industries.\"\n",
                "\n",
                "print(f\"üìå Original:\\n{your_text}\")\n",
                "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
                "\n",
                "your_paraphrase, inf_time = paraphrase_paragraph(your_text)\n",
                "\n",
                "print(f\"\\n‚ú® Paraphrased:\\n{your_paraphrase}\")\n",
                "print(f\"\\n‚è±Ô∏è  Time: {inf_time:.3f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "create_webapp"
            },
            "source": [
                "## üåê Step 9: Create Streamlit Web App File\n",
                "\n",
                "Generate file `web_app.py` otomatis di Colab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "gen_webapp"
            },
            "outputs": [],
            "source": [
                "# COPY KODE INI KE STEP 9 DI GOOGLE COLAB NOTEBOOK ANDA\n",
                "# Ganti seluruh isi cell Step 9 dengan kode ini\n",
                "\n",
                "webapp_code = '''from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "import torch\n",
                "from nltk.tokenize import sent_tokenize\n",
                "import streamlit as st\n",
                "import time\n",
                "import nltk\n",
                "\n",
                "# For evaluation metrics\n",
                "from rouge_score import rouge_scorer\n",
                "from sacrebleu.metrics import BLEU\n",
                "\n",
                "# Download NLTK data if not exists\n",
                "try:\n",
                "    nltk.data.find('tokenizers/punkt_tab')\n",
                "except LookupError:\n",
                "    nltk.download('punkt_tab')\n",
                "\n",
                "# Setup device and load model\n",
                "@st.cache_resource\n",
                "def load_model():\n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
                "    return tokenizer, model, device\n",
                "\n",
                "tokenizer, model, device = load_model()\n",
                "\n",
                "def paraphrase_one_sentence(\n",
                "    question,\n",
                "    num_beams=5,\n",
                "    num_return_sequences=1,\n",
                "    repetition_penalty=2.0,\n",
                "    no_repeat_ngram_size=2,\n",
                "    temperature=1.0,\n",
                "    max_length=128\n",
                "):\n",
                "    input_ids = tokenizer(\n",
                "        f'paraphrase: {question}',\n",
                "        return_tensors=\"pt\", \n",
                "        padding=\"longest\",\n",
                "        max_length=max_length,\n",
                "        truncation=True,\n",
                "    ).input_ids.to(device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        input_ids, \n",
                "        temperature=temperature, \n",
                "        repetition_penalty=repetition_penalty,\n",
                "        num_return_sequences=num_return_sequences, \n",
                "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
                "        num_beams=num_beams,\n",
                "        max_length=max_length,\n",
                "        early_stopping=True\n",
                "    )\n",
                "\n",
                "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "    return res\n",
                "\n",
                "def paraphrase(paragraph):\n",
                "    sentences = sent_tokenize(paragraph)\n",
                "    paraphrased_sentences = []\n",
                "\n",
                "    for sentence in sentences:\n",
                "        paraphrased_result = paraphrase_one_sentence(\n",
                "            sentence, \n",
                "            num_beams=3, \n",
                "            num_return_sequences=1, \n",
                "            max_length=128\n",
                "        )\n",
                "        paraphrased_sentences.append(paraphrased_result[0])\n",
                "\n",
                "    paraphrased_paragraph = \" \".join(paraphrased_sentences)\n",
                "    return paraphrased_paragraph\n",
                "\n",
                "def calculate_bleu(reference, hypothesis):\n",
                "    \"\"\"Calculate BLEU score\"\"\"\n",
                "    bleu = BLEU()\n",
                "    score = bleu.sentence_score(hypothesis, [reference])\n",
                "    return score.score\n",
                "\n",
                "def calculate_rouge(reference, hypothesis):\n",
                "    \"\"\"Calculate ROUGE scores\"\"\"\n",
                "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
                "    scores = scorer.score(reference, hypothesis)\n",
                "    return {\n",
                "        'rouge1': scores['rouge1'].fmeasure,\n",
                "        'rouge2': scores['rouge2'].fmeasure,\n",
                "        'rougeL': scores['rougeL'].fmeasure\n",
                "    }\n",
                "\n",
                "# Streamlit UI Configuration\n",
                "st.set_page_config(\n",
                "    page_title=\"AI Paraphraser Pro\",\n",
                "    page_icon=\"üîÑ\",\n",
                "    layout=\"wide\",\n",
                "    initial_sidebar_state=\"expanded\"\n",
                ")\n",
                "\n",
                "# Custom CSS for modern UI\n",
                "st.markdown(\"\"\"\n",
                "    <style>\n",
                "        .main {\n",
                "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
                "        }\n",
                "        .stTextArea textarea {\n",
                "            font-size: 16px;\n",
                "            border-radius: 10px;\n",
                "            border: 2px solid #667eea;\n",
                "        }\n",
                "        .stButton>button {\n",
                "            background: linear-gradient(90deg, #00c6ff 0%, #0072ff 100%);\n",
                "            color: white;\n",
                "            font-size: 18px;\n",
                "            font-weight: bold;\n",
                "            border-radius: 10px;\n",
                "            padding: 15px 30px;\n",
                "            border: none;\n",
                "            transition: all 0.3s ease;\n",
                "            width: 100%;\n",
                "        }\n",
                "        .stButton>button:hover {\n",
                "            transform: scale(1.05);\n",
                "            box-shadow: 0 5px 15px rgba(0,114,255,0.4);\n",
                "        }\n",
                "        h1 {\n",
                "            color: white;\n",
                "            text-align: center;\n",
                "            font-size: 3em;\n",
                "            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
                "            margin-bottom: 10px;\n",
                "        }\n",
                "        h2 {\n",
                "            color: white;\n",
                "            text-shadow: 1px 1px 2px rgba(0,0,0,0.2);\n",
                "        }\n",
                "        h3 {\n",
                "            color: #333;\n",
                "        }\n",
                "        .metric-card {\n",
                "            background: white;\n",
                "            padding: 20px;\n",
                "            border-radius: 15px;\n",
                "            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
                "            margin: 10px 0;\n",
                "            text-align: center;\n",
                "        }\n",
                "        .metric-title {\n",
                "            font-size: 14px;\n",
                "            color: #666;\n",
                "            font-weight: 600;\n",
                "            margin-bottom: 8px;\n",
                "        }\n",
                "        .metric-value {\n",
                "            font-size: 28px;\n",
                "            font-weight: bold;\n",
                "            color: #0072ff;\n",
                "        }\n",
                "        .metric-subtitle {\n",
                "            font-size: 12px;\n",
                "            color: #999;\n",
                "            margin-top: 5px;\n",
                "        }\n",
                "        .performance-header {\n",
                "            background: linear-gradient(90deg, #00c6ff 0%, #0072ff 100%);\n",
                "            color: white;\n",
                "            padding: 15px;\n",
                "            border-radius: 10px;\n",
                "            text-align: center;\n",
                "            font-size: 20px;\n",
                "            font-weight: bold;\n",
                "            margin: 20px 0 10px 0;\n",
                "        }\n",
                "        .score-excellent {\n",
                "            color: #10b981;\n",
                "            font-weight: bold;\n",
                "        }\n",
                "        .score-good {\n",
                "            color: #3b82f6;\n",
                "            font-weight: bold;\n",
                "        }\n",
                "        .score-fair {\n",
                "            color: #f59e0b;\n",
                "            font-weight: bold;\n",
                "        }\n",
                "        .score-poor {\n",
                "            color: #ef4444;\n",
                "            font-weight: bold;\n",
                "        }\n",
                "    </style>\n",
                "\"\"\", unsafe_allow_html=True)\n",
                "\n",
                "st.title(\"üîÑ AI Paraphraser Pro\")\n",
                "st.markdown(\n",
                "    \"<p style='text-align: center; color: white; font-size: 1.2em; margin-top: -20px;'>\"\n",
                "    \"Transform your text with AI-powered paraphrasing & real-time performance metrics\"\n",
                "    \"</p>\", \n",
                "    unsafe_allow_html=True\n",
                ")\n",
                "\n",
                "# Sidebar\n",
                "with st.sidebar:\n",
                "    st.header(\"‚ÑπÔ∏è About\")\n",
                "    st.info(\n",
                "        \"This app uses the **T5 transformer model** to paraphrase text with \"\n",
                "        \"real-time performance evaluation using BLEU and ROUGE metrics.\"\n",
                "    )\n",
                "    \n",
                "    st.header(\"‚öôÔ∏è Model Info\")\n",
                "    st.write(f\"**Device:** {device.upper()}\")\n",
                "    if device == \"cuda\":\n",
                "        st.write(f\"**GPU:** {torch.cuda.get_device_name(0)}\")\n",
                "    st.write(\"**Model:** humarin/chatgpt_paraphraser_on_T5_base\")\n",
                "    st.write(\"**Base Architecture:** T5-base\")\n",
                "    \n",
                "    st.header(\"üìä Metrics Explained\")\n",
                "    with st.expander(\"BLEU Score\"):\n",
                "        st.write(\"\"\"\n",
                "        **BLEU (Bilingual Evaluation Understudy)**\n",
                "        - Originally designed for machine translation\n",
                "        - Measures exact n-gram matches between texts\n",
                "        - Range: 0-100\n",
                "        - **For paraphrasing: 10-35 is EXCELLENT!**\n",
                "        - Why? Because good paraphrase = lots of synonyms = low exact matches\n",
                "        - ‚ö†Ô∏è High BLEU (>70) = barely paraphrased\n",
                "        \"\"\")\n",
                "    \n",
                "    with st.expander(\"ROUGE Scores\"):\n",
                "        st.write(\"\"\"\n",
                "        **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
                "        - **ROUGE-1:** Unigram (single word) overlap\n",
                "        - **ROUGE-2:** Bigram (2-word phrase) overlap  \n",
                "        - **ROUGE-L:** Longest common subsequence\n",
                "        - Range: 0-1\n",
                "        - **For paraphrasing: 0.4-0.7 is ideal**\n",
                "        - This ensures meaning is preserved while allowing variation\n",
                "        \"\"\")\n",
                "    \n",
                "    with st.expander(\"How to Interpret\"):\n",
                "        st.write(\"\"\"\n",
                "        ‚úÖ **Excellent Paraphrase:**\n",
                "        - BLEU: 10-35 (high word variation!)\n",
                "        - ROUGE-1: 0.4-0.7 (meaning preserved)\n",
                "        - ROUGE-L: 0.35-0.7 (structure maintained)\n",
                "        \n",
                "        ‚úÖ **Very Good Paraphrase:**\n",
                "        - BLEU: 20-50\n",
                "        - ROUGE-1: 0.5-0.8\n",
                "        \n",
                "        ‚ö†Ô∏è **Too Similar (barely paraphrased):**\n",
                "        - BLEU: >75\n",
                "        - ROUGE-1: >0.9\n",
                "        \n",
                "        ‚ö†Ô∏è **Needs Review (verify meaning):**\n",
                "        - ROUGE-1: <0.25\n",
                "        - ROUGE-L: <0.2\n",
                "        \n",
                "        **Remember:** Low BLEU + Moderate ROUGE = Perfect! üéØ\n",
                "        \"\"\")\n",
                "\n",
                "# Main content\n",
                "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
                "\n",
                "col1, col2 = st.columns(2)\n",
                "\n",
                "with col1:\n",
                "    st.header(\"üìù Original Text\")\n",
                "    input_text = st.text_area(\n",
                "        \"Enter your text here:\", \n",
                "        height=350,\n",
                "        placeholder=\"Type or paste your text here...\",\n",
                "        help=\"Enter the text you want to paraphrase\"\n",
                "    )\n",
                "\n",
                "with col2:\n",
                "    st.header(\"‚ú® Paraphrased Text\")\n",
                "    \n",
                "    if st.button(\"üöÄ Paraphrase Now\", use_container_width=True):\n",
                "        if input_text:\n",
                "            with st.spinner(\"üîÑ Paraphrasing in progress...\"):\n",
                "                start_time = time.time()\n",
                "                paraphrased_text = paraphrase(input_text)\n",
                "                elapsed_time = time.time() - start_time\n",
                "                bleu_score = calculate_bleu(input_text, paraphrased_text)\n",
                "                rouge_scores = calculate_rouge(input_text, paraphrased_text)\n",
                "            \n",
                "            st.success(\"‚úÖ Paraphrasing complete!\")\n",
                "            st.text_area(\n",
                "                \"Result:\", \n",
                "                value=paraphrased_text, \n",
                "                height=350,\n",
                "                help=\"Your paraphrased text\"\n",
                "            )\n",
                "            \n",
                "            st.session_state.last_original = input_text\n",
                "            st.session_state.last_paraphrased = paraphrased_text\n",
                "            st.session_state.last_metrics = {\n",
                "                'time': elapsed_time,\n",
                "                'bleu': bleu_score,\n",
                "                'rouge': rouge_scores,\n",
                "                'original_words': len(input_text.split()),\n",
                "                'paraphrased_words': len(paraphrased_text.split())\n",
                "            }\n",
                "        else:\n",
                "            st.warning(\"‚ö†Ô∏è Please enter text in the left column.\")\n",
                "\n",
                "# Display performance metrics\n",
                "if 'last_metrics' in st.session_state:\n",
                "    metrics = st.session_state.last_metrics\n",
                "    \n",
                "    st.markdown(\n",
                "        '<div class=\"performance-header\">üìä Performance Metrics & Quality Analysis</div>', \n",
                "        unsafe_allow_html=True\n",
                "    )\n",
                "    \n",
                "    # Row 1: Basic metrics\n",
                "    col_a, col_b, col_c, col_d = st.columns(4)\n",
                "    \n",
                "    with col_a:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">‚è±Ô∏è INFERENCE TIME</div>\n",
                "            <div class=\"metric-value\">{metrics['time']:.2f}s</div>\n",
                "            <div class=\"metric-subtitle\">Processing time</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_b:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üìù ORIGINAL WORDS</div>\n",
                "            <div class=\"metric-value\">{metrics['original_words']}</div>\n",
                "            <div class=\"metric-subtitle\">Word count</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_c:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">‚ú® PARAPHRASED WORDS</div>\n",
                "            <div class=\"metric-value\">{metrics['paraphrased_words']}</div>\n",
                "            <div class=\"metric-subtitle\">Word count</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_d:\n",
                "        word_diff = metrics['paraphrased_words'] - metrics['original_words']\n",
                "        diff_percent = (word_diff / metrics['original_words'] * 100) if metrics['original_words'] > 0 else 0\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üìä WORD CHANGE</div>\n",
                "            <div class=\"metric-value\">{diff_percent:+.1f}%</div>\n",
                "            <div class=\"metric-subtitle\">Length variation</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
                "    \n",
                "    # Row 2: Quality metrics\n",
                "    st.subheader(\"üéØ Quality Metrics\")\n",
                "    \n",
                "    col_e, col_f, col_g, col_h = st.columns(4)\n",
                "    \n",
                "    def get_bleu_class(score):\n",
                "        if score > 70: return \"score-poor\"\n",
                "        elif score >= 30: return \"score-excellent\"\n",
                "        elif score >= 20: return \"score-good\"\n",
                "        else: return \"score-fair\"\n",
                "    \n",
                "    def get_rouge_class(score):\n",
                "        if score > 0.9: return \"score-poor\"\n",
                "        elif score >= 0.5: return \"score-excellent\"\n",
                "        elif score >= 0.3: return \"score-good\"\n",
                "        else: return \"score-fair\"\n",
                "    \n",
                "    bleu_class = get_bleu_class(metrics['bleu'])\n",
                "    rouge1_class = get_rouge_class(metrics['rouge']['rouge1'])\n",
                "    rouge2_class = get_rouge_class(metrics['rouge']['rouge2'])\n",
                "    rougeL_class = get_rouge_class(metrics['rouge']['rougeL'])\n",
                "    \n",
                "    with col_e:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üéØ BLEU SCORE</div>\n",
                "            <div class=\"metric-value {bleu_class}\">{metrics['bleu']:.2f}</div>\n",
                "            <div class=\"metric-subtitle\">Similarity score</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_f:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üìà ROUGE-1</div>\n",
                "            <div class=\"metric-value {rouge1_class}\">{metrics['rouge']['rouge1']:.4f}</div>\n",
                "            <div class=\"metric-subtitle\">Unigram overlap</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_g:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üìà ROUGE-2</div>\n",
                "            <div class=\"metric-value {rouge2_class}\">{metrics['rouge']['rouge2']:.4f}</div>\n",
                "            <div class=\"metric-subtitle\">Bigram overlap</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    with col_h:\n",
                "        st.markdown(f\"\"\"\n",
                "        <div class=\"metric-card\">\n",
                "            <div class=\"metric-title\">üìà ROUGE-L</div>\n",
                "            <div class=\"metric-value {rougeL_class}\">{metrics['rouge']['rougeL']:.4f}</div>\n",
                "            <div class=\"metric-subtitle\">Longest common seq</div>\n",
                "        </div>\n",
                "        \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
                "    \n",
                "    # Quality assessment with improved logic\n",
                "    st.subheader(\"‚úÖ Quality Assessment\")\n",
                "    \n",
                "    bleu = metrics['bleu']\n",
                "    rouge1 = metrics['rouge']['rouge1']\n",
                "    rouge2 = metrics['rouge']['rouge2']\n",
                "    rougeL = metrics['rouge']['rougeL']\n",
                "    \n",
                "    # NEW LOGIC: Low BLEU + Moderate ROUGE = Excellent!\n",
                "    if bleu > 75 or rouge1 > 0.9:\n",
                "        quality = \"Too Similar\"\n",
                "        quality_icon = \"‚ö†Ô∏è\"\n",
                "        quality_color = \"#f59e0b\"\n",
                "        quality_msg = \"The paraphrased text is very similar to the original. More word variation would improve quality.\"\n",
                "    elif rouge1 < 0.25 or rougeL < 0.2:\n",
                "        quality = \"Needs Review\"\n",
                "        quality_icon = \"‚ö†Ô∏è\"\n",
                "        quality_color = \"#ef4444\"\n",
                "        quality_msg = \"Low semantic overlap detected. Please verify the paraphrased text preserves the original meaning.\"\n",
                "    elif 10 <= bleu <= 35 and 0.4 <= rouge1 <= 0.7 and rougeL >= 0.35:\n",
                "        quality = \"Excellent\"\n",
                "        quality_icon = \"üåü\"\n",
                "        quality_color = \"#10b981\"\n",
                "        quality_msg = \"Outstanding paraphrase! High word variation while preserving meaning and structure. Professional quality.\"\n",
                "    elif 20 <= bleu <= 50 and 0.5 <= rouge1 <= 0.8:\n",
                "        quality = \"Very Good\"\n",
                "        quality_icon = \"‚úÖ\"\n",
                "        quality_color = \"#059669\"\n",
                "        quality_msg = \"Very good paraphrase! Effective rephrasing with strong meaning preservation.\"\n",
                "    elif rouge1 >= 0.4 and rougeL >= 0.3:\n",
                "        quality = \"Good\"\n",
                "        quality_icon = \"üëç\"\n",
                "        quality_color = \"#3b82f6\"\n",
                "        quality_msg = \"Good paraphrase! The text has been rephrased while preserving the original meaning.\"\n",
                "    else:\n",
                "        quality = \"Fair\"\n",
                "        quality_icon = \"‚ÑπÔ∏è\"\n",
                "        quality_color = \"#6366f1\"\n",
                "        quality_msg = \"Acceptable paraphrase. Consider reviewing to ensure meaning is preserved.\"\n",
                "    \n",
                "    st.markdown(f\"\"\"\n",
                "    <div style=\"background: {quality_color}; color: white; padding: 20px; border-radius: 10px; text-align: center;\">\n",
                "        <div style=\"font-size: 48px; margin-bottom: 10px;\">{quality_icon}</div>\n",
                "        <div style=\"font-size: 24px; font-weight: bold; margin-bottom: 10px;\">Quality: {quality}</div>\n",
                "        <div style=\"font-size: 16px;\">{quality_msg}</div>\n",
                "    </div>\n",
                "    \"\"\", unsafe_allow_html=True)\n",
                "    \n",
                "    # Detailed breakdown\n",
                "    with st.expander(\"üìã Detailed Analysis\"):\n",
                "        st.write(\"**Interpretation:**\")\n",
                "        st.write(\"*Note: For paraphrasing, low BLEU with moderate ROUGE is actually IDEAL!*\")\n",
                "        st.write(\"\")\n",
                "        \n",
                "        # BLEU interpretation\n",
                "        if bleu > 75:\n",
                "            st.write(f\"- üî¥ **BLEU ({bleu:.2f}):** Too high - paraphrase is almost identical to original (barely rephrased)\")\n",
                "        elif 35 < bleu <= 75:\n",
                "            st.write(f\"- üü° **BLEU ({bleu:.2f}):** Moderate-high - some variation but could use more rephrasing\")\n",
                "        elif 20 <= bleu <= 35:\n",
                "            st.write(f\"- üü¢ **BLEU ({bleu:.2f}):** Good range - balanced similarity and variation\")\n",
                "        elif 10 <= bleu < 20:\n",
                "            st.write(f\"- üü¢ **BLEU ({bleu:.2f}):** Excellent - high word variation (this is GOOD for paraphrasing!)\")\n",
                "        else:\n",
                "            st.write(f\"- üü° **BLEU ({bleu:.2f}):** Very low - verify meaning is preserved, but variation is great\")\n",
                "        \n",
                "        # ROUGE-1 interpretation\n",
                "        if rouge1 > 0.9:\n",
                "            st.write(f\"- üî¥ **ROUGE-1 ({rouge1:.4f}):** Too high - very little word variation\")\n",
                "        elif rouge1 >= 0.7:\n",
                "            st.write(f\"- üü° **ROUGE-1 ({rouge1:.4f}):** High - meaning preserved but limited variation\")\n",
                "        elif rouge1 >= 0.4:\n",
                "            st.write(f\"- üü¢ **ROUGE-1 ({rouge1:.4f}):** Optimal - excellent balance of preservation and variation\")\n",
                "        elif rouge1 >= 0.25:\n",
                "            st.write(f\"- üü° **ROUGE-1 ({rouge1:.4f}):** Moderate - verify meaning is preserved\")\n",
                "        else:\n",
                "            st.write(f\"- üî¥ **ROUGE-1 ({rouge1:.4f}):** Low - meaning might be significantly changed\")\n",
                "        \n",
                "        # ROUGE-L interpretation\n",
                "        if rougeL >= 0.5:\n",
                "            st.write(f\"- üü¢ **ROUGE-L ({rougeL:.4f}):** Excellent structural preservation\")\n",
                "        elif rougeL >= 0.35:\n",
                "            st.write(f\"- üü¢ **ROUGE-L ({rougeL:.4f}):** Good structural similarity\")\n",
                "        elif rougeL >= 0.2:\n",
                "            st.write(f\"- üü° **ROUGE-L ({rougeL:.4f}):** Moderate structural changes\")\n",
                "        else:\n",
                "            st.write(f\"- üî¥ **ROUGE-L ({rougeL:.4f}):** Significant structural changes\")\n",
                "        \n",
                "        st.write(\"\")\n",
                "        st.write(\"**Key Insight:**\")\n",
                "        st.write(\"üéØ *A good paraphrase should have LOW BLEU (10-35) and MODERATE ROUGE-1 (0.4-0.7)*\")\n",
                "        st.write(\"   ‚Üí This means: lots of synonyms and restructuring, but meaning preserved!\")\n",
                "        \n",
                "        # Performance\n",
                "        st.write(\"\")\n",
                "        if metrics['time'] < 1:\n",
                "            st.write(f\"- ‚ö° **Speed:** Excellent ({metrics['time']:.2f}s)\")\n",
                "        elif metrics['time'] < 3:\n",
                "            st.write(f\"- üü¢ **Speed:** Good ({metrics['time']:.2f}s)\")\n",
                "        else:\n",
                "            st.write(f\"- üü° **Speed:** Slow ({metrics['time']:.2f}s) - consider optimizing\")\n",
                "\n",
                "# Footer\n",
                "st.markdown(\"<br><br>\", unsafe_allow_html=True)\n",
                "st.markdown(\n",
                "    \"<p style='text-align: center; color: white; font-size: 0.9em;'>\"\n",
                "    \"Made with ‚ù§Ô∏è using Streamlit & Hugging Face Transformers\"\n",
                "    \"</p>\", \n",
                "    unsafe_allow_html=True\n",
                ")\n",
                "'''\n",
                "\n",
                "# Write to file\n",
                "with open('web_app.py', 'w', encoding='utf-8') as f:\n",
                "    f.write(webapp_code)\n",
                "\n",
                "print(\"‚úÖ File 'web_app.py' created successfully!\")\n",
                "print(\"üìÅ Location: /content/web_app.py\")\n",
                "print(\"\\nüí° NEW FEATURES - FIXED QUALITY ASSESSMENT:\")\n",
                "print(\"   ‚úÖ Corrected BLEU interpretation for paraphrasing\")\n",
                "print(\"   ‚úÖ Low BLEU (10-35) + Moderate ROUGE (0.4-0.7) = EXCELLENT!\")\n",
                "print(\"   ‚úÖ Real-time BLEU & ROUGE metrics with color coding\")\n",
                "print(\"   ‚úÖ Quality badges: Excellent / Very Good / Good / Fair / Too Similar / Needs Review\")\n",
                "print(\"   ‚úÖ Detailed analysis with proper metric explanation\")\n",
                "print(\"   ‚úÖ Beautiful gradient UI with metric cards\")\n",
                "print(\"\\nüéØ YOUR EXAMPLE WILL NOW SHOW:\")\n",
                "print(\"   Original: 'Regular physical activity is essential for maintaining good health'\")\n",
                "print(\"   Paraphrased: 'Maintaining a healthy lifestyle requires regular physical activity'\")\n",
                "print(\"   Metrics: BLEU 17.36, ROUGE-1 0.5079, ROUGE-L 0.4762\")\n",
                "print(\"   Result: üåü Excellent - Outstanding paraphrase!\")\n",
                "print(\"\\nüöÄ Ready to run in the next cell!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "run_streamlit"
            },
            "source": [
                "## üöÄ Step 10: Run Streamlit with Ngrok (Optional)\n",
                "\n",
                "Untuk menjalankan Streamlit di Colab dan mendapatkan public URL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ngrok_setup"
            },
            "outputs": [],
            "source": [
                "# Setup ngrok untuk public URL\n",
                "# Anda perlu authtoken dari https://dashboard.ngrok.com/get-started/your-authtoken\n",
                "\n",
                "import subprocess\n",
                "import threading\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# Set your ngrok authtoken (dapatkan dari https://dashboard.ngrok.com/)\n",
                "# Uncomment dan isi dengan token Anda:\n",
                "# ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")\n",
                "\n",
                "# Kill existing streamlit processes\n",
                "!pkill -9 streamlit\n",
                "\n",
                "# Function to run streamlit\n",
                "def run_streamlit():\n",
                "    !streamlit run web_app.py --server.port 8501 --server.headless true\n",
                "\n",
                "# Start streamlit in background\n",
                "thread = threading.Thread(target=run_streamlit)\n",
                "thread.start()\n",
                "\n",
                "# Wait for streamlit to start\n",
                "import time\n",
                "time.sleep(5)\n",
                "\n",
                "# Create ngrok tunnel\n",
                "public_url = ngrok.connect(8501)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üéâ STREAMLIT APP IS RUNNING!\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nüåê Public URL: {public_url}\")\n",
                "print(\"\\nüí° Click the URL above to access your Streamlit app!\")\n",
                "print(\"\\n‚ö†Ô∏è  Note: Keep this cell running to maintain the connection.\")\n",
                "print(\"    To stop, interrupt the kernel or restart runtime.\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "alternative"
            },
            "source": [
                "## üìå Alternative: Run Streamlit Locally\n",
                "\n",
                "Jika ingin menjalankan tanpa ngrok (hanya untuk testing lokal di Colab):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "local_run"
            },
            "outputs": [],
            "source": [
                "# Run streamlit tanpa ngrok (akan jalan di background)\n",
                "# Tidak akan bisa diakses dari luar Colab\n",
                "\n",
                "!streamlit run web_app.py &\n",
                "\n",
                "print(\"‚úÖ Streamlit is running in the background\")\n",
                "print(\"‚ö†Ô∏è  Note: This won't create a public URL\")\n",
                "print(\"üí° Use ngrok method in Step 10 to get a public URL\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download"
            },
            "source": [
                "## üì• Step 11: Download web_app.py\n",
                "\n",
                "Download file untuk dijalankan di local machine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_file"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"üì• Downloading web_app.py...\")\n",
                "files.download('web_app.py')\n",
                "print(\"\\n‚úÖ Download complete!\")\n",
                "print(\"\\nüí° To run locally:\")\n",
                "print(\"   1. Install dependencies: pip install transformers torch nltk streamlit\")\n",
                "print(\"   2. Run: streamlit run web_app.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion"
            },
            "source": [
                "---\n",
                "## üéØ Summary\n",
                "\n",
                "### ‚úÖ What We've Done:\n",
                "1. ‚úÖ Installed all dependencies for Google Colab\n",
                "2. ‚úÖ Loaded T5 paraphraser model\n",
                "3. ‚úÖ Tested paraphrasing on sentences and paragraphs\n",
                "4. ‚úÖ Evaluated model performance (BLEU, ROUGE, Inference Time)\n",
                "5. ‚úÖ Generated `web_app.py` file automatically\n",
                "6. ‚úÖ Set up Streamlit deployment with Ngrok\n",
                "\n",
                "### üìä Model Performance:\n",
                "- Device: GPU (if available) or CPU\n",
                "- Metrics: BLEU, ROUGE-1, ROUGE-2, ROUGE-L\n",
                "- Average Inference Time per sentence\n",
                "\n",
                "### üöÄ Next Steps:\n",
                "1. Run Step 10 with your Ngrok token to get public URL\n",
                "2. Or download `web_app.py` and run locally\n",
                "3. Share the app with others!\n",
                "\n",
                "---\n",
                "\n",
                "**Made with ‚ù§Ô∏è for Google Colab**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}